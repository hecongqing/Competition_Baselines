{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut(sentence):\n",
    "    \"\"\"\n",
    "    将一段文本切分成多个句子\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentence = []\n",
    "    sen = []\n",
    "    for i in sentence:\n",
    "        if i in ['。', '！', '？', '?'] and len(sen) != 0:\n",
    "            sen.append(i)\n",
    "            new_sentence.append(\"\".join(sen))\n",
    "            sen = []\n",
    "            continue\n",
    "        sen.append(i)\n",
    "\n",
    "    if len(new_sentence) <= 1: # 一句话超过max_seq_length且没有句号的，用\",\"分割，再长的不考虑了。\n",
    "        new_sentence = []\n",
    "        sen = []\n",
    "        for i in sentence:\n",
    "            if i.split(' ')[0] in ['，', ','] and len(sen) != 0:\n",
    "                sen.append(i)\n",
    "                new_sentence.append(\"\".join(sen))\n",
    "                sen = []\n",
    "                continue\n",
    "            sen.append(i)\n",
    "    if len(sen) > 0:  # 若最后一句话无结尾标点，则加入这句话\n",
    "        new_sentence.append(\"\".join(sen))\n",
    "    return new_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_test_set(text_list,len_treshold):\n",
    "    cut_text_list = []\n",
    "    cut_index_list = []\n",
    "    for text in text_list:\n",
    "\n",
    "        temp_cut_text_list = []\n",
    "        text_agg = ''\n",
    "        if len(text) < len_treshold:\n",
    "            temp_cut_text_list.append(text)\n",
    "        else:\n",
    "            sentence_list = _cut(text)  # 一条数据被切分成多句话\n",
    "            for sentence in sentence_list:\n",
    "                if len(text_agg) + len(sentence) < len_treshold:\n",
    "                    text_agg += sentence\n",
    "                else:\n",
    "                    temp_cut_text_list.append(text_agg)\n",
    "                    text_agg = sentence\n",
    "            temp_cut_text_list.append(text_agg)  # 加上最后一个句子\n",
    "\n",
    "        cut_index_list.append(len(temp_cut_text_list))\n",
    "        cut_text_list += temp_cut_text_list\n",
    "\n",
    "    return cut_text_list, cut_index_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置样本长度\n",
    "text_length = 250\n",
    "def from_ann2dic(r_ann_path, r_txt_path, w_path, w_file):\n",
    "    q_dic = {}\n",
    "    with codecs.open(r_ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "            line_arr = line.split('\\t')\n",
    "            entityinfo = line_arr[1]\n",
    "            entityinfo = entityinfo.split(' ')\n",
    "            cls = entityinfo[0]\n",
    "            start_index = int(entityinfo[1])\n",
    "            end_index = int(entityinfo[2])\n",
    "            length = end_index - start_index\n",
    "            for r in range(length):\n",
    "                if r == 0:\n",
    "                    q_dic[start_index] = (\"B-%s\" % cls)\n",
    "                else:\n",
    "                    q_dic[start_index + r] = (\"I-%s\" % cls)\n",
    "\n",
    "    with codecs.open(r_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content_str = f.read()\n",
    "        \n",
    "    \n",
    "    cut_text_list, cut_index_list = cut_test_set([content_str],text_length)\n",
    "    \n",
    "    i = 0\n",
    "    for idx, line in enumerate(cut_text_list):\n",
    "        w_path_ = \"%s/%s-%s-new.txt\" % (w_path, w_file,idx)\n",
    "        with codecs.open(w_path_, \"w\", encoding=\"utf-8\") as w:\n",
    "            for str_ in line:\n",
    "                if str_ is \" \" or str_ == \"\" or str_ == \"\\n\" or str_ == \"\\r\":\n",
    "                    pass\n",
    "                else:\n",
    "                    if i in q_dic:\n",
    "                        tag = q_dic[i]\n",
    "                    else:\n",
    "                        tag = \"O\"  # 大写字母O\n",
    "                    w.write('%s %s\\n' % (str_, tag))\n",
    "                i+=1\n",
    "            w.write('%s\\n' % \"END O\")            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list = glob.glob('./round1_train/train/*.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filelist, val_filelist = train_test_split(file_list,test_size=0.2,random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir  ./round1_train/train_new/\n",
    "!mkdir ./round1_train/val_new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "data_dir = './round1_train/train/'\n",
    "for file in train_filelist:\n",
    "    if file.find(\".ann\") == -1 and file.find(\".txt\") == -1:\n",
    "        continue\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(data_dir, \"%s.ann\" % file_name)\n",
    "    r_txt_path = os.path.join(data_dir, \"%s.txt\" % file_name)\n",
    "    w_path = './round1_train/train_new/'\n",
    "    w_file = file_name\n",
    "    from_ann2dic(r_ann_path, r_txt_path, w_path,w_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "data_dir = './round1_train/train/'\n",
    "for file in val_filelist:\n",
    "    if file.find(\".ann\") == -1 and file.find(\".txt\") == -1:\n",
    "        continue\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(data_dir, \"%s.ann\" % file_name)\n",
    "    r_txt_path = os.path.join(data_dir, \"%s.txt\" % file_name)\n",
    "    w_path = './round1_train/val_new/'\n",
    "    w_file = file_name\n",
    "    from_ann2dic(r_ann_path, r_txt_path, w_path,w_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_path = \"./round1_train/data/train.txt\"\n",
    "for file in os.listdir('./round1_train/train_new/'):\n",
    "    path = os.path.join(\"./round1_train/train_new\", file)\n",
    "    if not file.endswith(\".txt\"):  \n",
    "        continue\n",
    "    q_list = []\n",
    "    print(\"开始读取文件:%s\" % file)\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip(\"\\n\\r\")\n",
    "        while line != \"END O\":\n",
    "            q_list.append(line)\n",
    "            line = f.readline()\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "    print(\"开始写入文本%s\" % w_path)\n",
    "    with codecs.open(w_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in q_list:\n",
    "            if item.__contains__('\\ufeff1'):\n",
    "                print(\"===============\")\n",
    "            f.write('%s\\n' % item)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_path = \"./round1_train/data/val.txt\"\n",
    "for file in os.listdir('./round1_train/val_new/'):\n",
    "    path = os.path.join(\"./round1_train/val_new\", file)\n",
    "    if not file.endswith(\".txt\"):  \n",
    "        continue\n",
    "    q_list = []\n",
    "\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip(\"\\n\\r\")\n",
    "        while line != \"END O\":\n",
    "            q_list.append(line)\n",
    "            line = f.readline()\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "    \n",
    "    with codecs.open(w_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in q_list:\n",
    "            if item.__contains__('\\ufeff1'):\n",
    "                print(\"===============\")\n",
    "            f.write('%s\\n' % item)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始验证集拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./round1_train/train/120.txt\n",
      "./round1_train/train/988.txt\n",
      "./round1_train/train/504.txt\n",
      "./round1_train/train/618.txt\n",
      "./round1_train/train/968.txt\n",
      "./round1_train/train/989.txt\n",
      "./round1_train/train/719.txt\n",
      "./round1_train/train/263.txt\n",
      "./round1_train/train/964.txt\n",
      "./round1_train/train/184.txt\n",
      "./round1_train/train/519.txt\n",
      "./round1_train/train/307.txt\n",
      "./round1_train/train/234.txt\n",
      "./round1_train/train/447.txt\n",
      "./round1_train/train/677.txt\n",
      "./round1_train/train/839.txt\n",
      "./round1_train/train/116.txt\n",
      "./round1_train/train/253.txt\n",
      "./round1_train/train/654.txt\n",
      "./round1_train/train/281.txt\n",
      "./round1_train/train/59.txt\n",
      "./round1_train/train/627.txt\n",
      "./round1_train/train/8.txt\n",
      "./round1_train/train/77.txt\n",
      "./round1_train/train/948.txt\n",
      "./round1_train/train/480.txt\n",
      "./round1_train/train/877.txt\n",
      "./round1_train/train/930.txt\n",
      "./round1_train/train/415.txt\n",
      "./round1_train/train/778.txt\n",
      "./round1_train/train/437.txt\n",
      "./round1_train/train/186.txt\n",
      "./round1_train/train/283.txt\n",
      "./round1_train/train/318.txt\n",
      "./round1_train/train/672.txt\n",
      "./round1_train/train/26.txt\n",
      "./round1_train/train/812.txt\n",
      "./round1_train/train/970.txt\n",
      "./round1_train/train/549.txt\n",
      "./round1_train/train/986.txt\n",
      "./round1_train/train/194.txt\n",
      "./round1_train/train/465.txt\n",
      "./round1_train/train/648.txt\n",
      "./round1_train/train/556.txt\n",
      "./round1_train/train/991.txt\n",
      "./round1_train/train/570.txt\n",
      "./round1_train/train/292.txt\n",
      "./round1_train/train/634.txt\n",
      "./round1_train/train/481.txt\n",
      "./round1_train/train/906.txt\n",
      "./round1_train/train/831.txt\n",
      "./round1_train/train/468.txt\n",
      "./round1_train/train/113.txt\n",
      "./round1_train/train/755.txt\n",
      "./round1_train/train/700.txt\n",
      "./round1_train/train/146.txt\n",
      "./round1_train/train/89.txt\n",
      "./round1_train/train/104.txt\n",
      "./round1_train/train/911.txt\n",
      "./round1_train/train/606.txt\n",
      "./round1_train/train/768.txt\n",
      "./round1_train/train/912.txt\n",
      "./round1_train/train/319.txt\n",
      "./round1_train/train/237.txt\n",
      "./round1_train/train/389.txt\n",
      "./round1_train/train/68.txt\n",
      "./round1_train/train/360.txt\n",
      "./round1_train/train/219.txt\n",
      "./round1_train/train/487.txt\n",
      "./round1_train/train/102.txt\n",
      "./round1_train/train/943.txt\n",
      "./round1_train/train/810.txt\n",
      "./round1_train/train/897.txt\n",
      "./round1_train/train/269.txt\n",
      "./round1_train/train/738.txt\n",
      "./round1_train/train/994.txt\n",
      "./round1_train/train/406.txt\n",
      "./round1_train/train/4.txt\n",
      "./round1_train/train/464.txt\n",
      "./round1_train/train/369.txt\n",
      "./round1_train/train/55.txt\n",
      "./round1_train/train/350.txt\n",
      "./round1_train/train/370.txt\n",
      "./round1_train/train/593.txt\n",
      "./round1_train/train/721.txt\n",
      "./round1_train/train/305.txt\n",
      "./round1_train/train/24.txt\n",
      "./round1_train/train/298.txt\n",
      "./round1_train/train/841.txt\n",
      "./round1_train/train/189.txt\n",
      "./round1_train/train/336.txt\n",
      "./round1_train/train/477.txt\n",
      "./round1_train/train/761.txt\n",
      "./round1_train/train/400.txt\n",
      "./round1_train/train/95.txt\n",
      "./round1_train/train/136.txt\n",
      "./round1_train/train/196.txt\n",
      "./round1_train/train/138.txt\n",
      "./round1_train/train/591.txt\n",
      "./round1_train/train/459.txt\n",
      "./round1_train/train/814.txt\n",
      "./round1_train/train/341.txt\n",
      "./round1_train/train/178.txt\n",
      "./round1_train/train/147.txt\n",
      "./round1_train/train/905.txt\n",
      "./round1_train/train/975.txt\n",
      "./round1_train/train/704.txt\n",
      "./round1_train/train/101.txt\n",
      "./round1_train/train/375.txt\n",
      "./round1_train/train/713.txt\n",
      "./round1_train/train/153.txt\n",
      "./round1_train/train/879.txt\n",
      "./round1_train/train/433.txt\n",
      "./round1_train/train/924.txt\n",
      "./round1_train/train/273.txt\n",
      "./round1_train/train/699.txt\n",
      "./round1_train/train/436.txt\n",
      "./round1_train/train/262.txt\n",
      "./round1_train/train/847.txt\n",
      "./round1_train/train/889.txt\n",
      "./round1_train/train/780.txt\n",
      "./round1_train/train/890.txt\n",
      "./round1_train/train/182.txt\n",
      "./round1_train/train/706.txt\n",
      "./round1_train/train/114.txt\n",
      "./round1_train/train/28.txt\n",
      "./round1_train/train/641.txt\n",
      "./round1_train/train/832.txt\n",
      "./round1_train/train/923.txt\n",
      "./round1_train/train/794.txt\n",
      "./round1_train/train/457.txt\n",
      "./round1_train/train/767.txt\n",
      "./round1_train/train/993.txt\n",
      "./round1_train/train/874.txt\n",
      "./round1_train/train/308.txt\n",
      "./round1_train/train/945.txt\n",
      "./round1_train/train/789.txt\n",
      "./round1_train/train/149.txt\n",
      "./round1_train/train/536.txt\n",
      "./round1_train/train/463.txt\n",
      "./round1_train/train/835.txt\n",
      "./round1_train/train/983.txt\n",
      "./round1_train/train/550.txt\n",
      "./round1_train/train/376.txt\n",
      "./round1_train/train/381.txt\n",
      "./round1_train/train/402.txt\n",
      "./round1_train/train/894.txt\n",
      "./round1_train/train/995.txt\n",
      "./round1_train/train/110.txt\n",
      "./round1_train/train/315.txt\n",
      "./round1_train/train/962.txt\n",
      "./round1_train/train/32.txt\n",
      "./round1_train/train/987.txt\n",
      "./round1_train/train/515.txt\n",
      "./round1_train/train/532.txt\n",
      "./round1_train/train/792.txt\n",
      "./round1_train/train/807.txt\n",
      "./round1_train/train/562.txt\n",
      "./round1_train/train/664.txt\n",
      "./round1_train/train/266.txt\n",
      "./round1_train/train/684.txt\n",
      "./round1_train/train/800.txt\n",
      "./round1_train/train/118.txt\n",
      "./round1_train/train/267.txt\n",
      "./round1_train/train/958.txt\n",
      "./round1_train/train/166.txt\n",
      "./round1_train/train/228.txt\n",
      "./round1_train/train/635.txt\n",
      "./round1_train/train/918.txt\n",
      "./round1_train/train/249.txt\n",
      "./round1_train/train/762.txt\n",
      "./round1_train/train/851.txt\n",
      "./round1_train/train/338.txt\n",
      "./round1_train/train/702.txt\n",
      "./round1_train/train/411.txt\n",
      "./round1_train/train/817.txt\n",
      "./round1_train/train/407.txt\n",
      "./round1_train/train/878.txt\n",
      "./round1_train/train/935.txt\n",
      "./round1_train/train/31.txt\n",
      "./round1_train/train/833.txt\n",
      "./round1_train/train/875.txt\n",
      "./round1_train/train/478.txt\n",
      "./round1_train/train/209.txt\n",
      "./round1_train/train/612.txt\n",
      "./round1_train/train/940.txt\n",
      "./round1_train/train/73.txt\n",
      "./round1_train/train/0.txt\n",
      "./round1_train/train/837.txt\n",
      "./round1_train/train/508.txt\n",
      "./round1_train/train/669.txt\n",
      "./round1_train/train/221.txt\n",
      "./round1_train/train/168.txt\n",
      "./round1_train/train/348.txt\n",
      "./round1_train/train/829.txt\n",
      "./round1_train/train/931.txt\n",
      "./round1_train/train/729.txt\n",
      "./round1_train/train/35.txt\n",
      "./round1_train/train/900.txt\n",
      "./round1_train/train/313.txt\n"
     ]
    }
   ],
   "source": [
    "for file in val_filelist:\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(\"./round1_train/train\", \"%s.ann\" % file_name)\n",
    "    os.system(\"cp %s %s\"%(file,\"./round1_train/val_data\"))\n",
    "    os.system(\"cp %s %s\"%(r_ann_path,\"./round1_train/val_data\"))\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
